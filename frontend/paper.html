<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Automatic Speech Recognition for the Ika Language</title>
</head>
<body>
    <header>
        <h1>AUTOMATIC SPEECH RECOGNITION FOR THE IKA LANGUAGE</h1>
        <h3>OGBUIGWE DANIEL UDOKA</h3>
        <p>Department of Computer Science</p>
        <p>School of Computing and Engineering Sciences</p>
        <p>Babcock University, Ilishan-Remo</p>
    </header>

    <section>
        <h2>ABSTRACT</h2>
        <p>We developed an automatic speech recognition model for the Ika language using just 1hr of labelled data. Deep Learning models are fast approaching human level performance in recognizing and transcribing speech. However, these highly capable models are limited to only a few of the 7000 living languages in the world. OpenAI’s Whisper model officially supports about 100 languages. The iPhone and Google Translate support 40 and ~300 languages respectively. In this paper, we show that fine-tuning pretrained wav2vec2.0 models can help quickly bring the coverage of speech recognition technology to thousands of languages.</p>
        <p>Keywords: Ika language, Massively Multilingual Speech (MMS), Automatic Speech Recognition, low-resource languages, wav2vec 2.0, language preservation.</p>
    </section>

    <section>
        <h2>INTRODUCTION</h2>
        <p>Machines today can recognize and understand human speech from diverse demographics. Regardless of the age or gender of the speaker, models like Whisper from OpenAI are approaching human level accuracy at recognizing and transcribing speech. Due to a gap in data resources, these highly capable deep learning models only cover about 4% of the 7000 living languages in the world. Researchers argue that a large portion of these languages will be forgotten by the end of the century. Our research explores feasible methods of developing language technology for the Ika language. Ika is a dialect of Igbo (igboid) spoken in Ika South and Ika Northeast Local Government Areas of Delta State and the Igbanke area of Edo State in Nigeria. It belongs to the Niger-Congo family of languages spoken in areas bordering the west of the River Niger. The motivations for this research are to create tools that language experts can use now for creating more representative technology for a language with a rich culture and history.</p>
        <p>Meta AI's Massively Multilingual Speech (MMS) project has made significant strides towards expanding language coverage for Automatic Speech Recognition. They used their self-supervised framework wav2vec 2.0 and a massive dataset of 500,000 hours of speech data to pretrain Massively Multilingual Speech models capable of speech recognition for 1107 languages. The multilingual pretrained models from Meta present a feasible opportunity to create a speech recognition system for the Ika language. Baevski demonstrated that wav2vec 2.0 was able to achieve 4.8/8.2 WER on the LibriSpeech test dataset using just ten minutes of labelled data. This is a huge indication that despite the labelled data discrepancy faced by the Ika language, we can utilize the limited speech data available to train a model to transcribe naturally spoken Ika language.</p>
        <p>The Ika language lacks developed language technology tools for speech recognition, synthesis, and machine translation. This hinders digital communication, education, and preservation efforts for Ika speakers in their preferred language. The rich speech representations within multilingual pretrained models offer a promising avenue for creating foundational language technology for Ika. It also provides a means for examining the current state of language technology for Ika. To investigate the potential of this approach, it's necessary to systematically assess how fine-tuning MMS models with Ika-specific data influences their performance. However, the effectiveness of fine-tuning MMS models specifically for low-resource languages like Ika needs further exploration.</p>
        <p>Research Question: How many minutes/hours of labelled Ika speech do we need to train a model to recognize Ika speech and how does the quantity of data affect the final model’s performance? We also hope to explore how the parameter size of the model affects its performance.</p>
        <p>This study will provide valuable insights to guide future development of language technology for Ika, supporting language preservation and wider access to digital resources. This study aimed to fine-tune Meta's massively multilingual pretrained Wav2Vec 2.0 models for automatic speech recognition (ASR) on the low-resource Ika language. The key goals were:</p>
        <ul>
            <li>Curate a dataset of transcribed Ika speech audio from religious texts to facilitate model fine-tuning.</li>
            <li>Fine-tune and evaluate the performance of Meta's 300 million and 1 billion parameter pretrained models on the Ika ASR task using the curated dataset.</li>
            <li>Analyse the impact of model size and pretraining on low-resource speech recognition capabilities by comparing the two models' results.</li>
            <li>Identify effective techniques and methodologies for adapting large pretrained speech models to low-resource language settings.</li>
            <li>Document the complete process including data preparation, model fine-tuning, evaluation metrics, and potential limitations.</li>
        </ul>
        <p>While initial hypotheses anticipated WER improvements with more data, the study's focus shifted to understanding how pretrained model capacities and fine-tuning strategies could compensate for limited data availability in low-resource languages like Ika. The specific measurable outcomes were the Word Error Rates (WERs) achieved by the fine-tuned models on held-out Ika speech test sets, analysed in the context of the training data size and model scale.</p>
    </section>

    <section>
        <h2>RELATED WORK</h2>
        <p>There is ample research available on speech recognition. Transformers and modern deep learning models have become the popular choice for most speech tasks. However, there is little to no research on the Ika language and limited research on similar languages in the Volta-Niger family of languages. We explore literature that involves developing speech technology for languages with limited resources. Certain techniques and strategies have been developed to address the gaps in data availability. Some of these techniques include cross-lingual speech representation learning, transfer learning, and data augmentation to name a few of what we will explore.</p>
        <h3>Cross-Lingual Representation Learning for Speech Recognition</h3>
        <p>Conneau et al. (2020) present XLSR, a model designed for cross-lingual speech representation learning. By pretraining on raw speech waveforms from multiple languages, XLSR significantly improves performance over monolingual models. Specifically, XLSR reduces the phoneme error rate by 72% on the CommonVoice benchmark and improves the word error rate by 16% on the BABEL dataset. This cross-lingual pretraining method shows the potential to enhance speech recognition, particularly for languages like Ika. The research suggests that applying such a model to Ika could leverage the shared latent discrete speech representations across languages, potentially overcoming the limitations of monolingual pretraining that requires a lot of data.</p>
        <h3>Low-Resource Language ASR</h3>
        <p>Seo et al. (2021) develop the first ASR model for Manchu leveraging Wav2Vec2-XLSR-53. This is very similar to our research which aims to develop a brand new ASR model for Ika. Their approach addresses the data scarcity through data augmentation resulting in a significant reduction in Character Error Rate (CER) and Word Error Rate (WER). They augment their data by adding background noise and reverberation to audio samples, clipping audio, and randomly removing segments from the audio. This study demonstrates the effectiveness of fine-tuning pretrained models with augmented data for extremely low-resource languages. For our research, similar data augmentation techniques could be employed to enhance ASR performance, ensuring the model is robust despite limited training data.</p>
        <h3>Fine-Tuning Self-Supervised Models for Non-ASR Tasks</h3>
        <p>Wang et al. (2021) explore the application of wav2vec 2.0 and HuBERT models to tasks beyond speech recognition. Their research explores tasks such as Speech Emotion Recognition, Speaker Verification, and Spoken Language Understanding. They demonstrate that fine-tuning these models can achieve state-of-the-art results indicating their versatility. This suggests that advanced fine-tuning strategies could be beneficial for developing comprehensive speech technology for Ika encompassing not only ASR but also other speech-related tasks. However, we will focus on ASR for this paper.</p>
        <h3>Adapter-Based Tuning for Pretrained Models</h3>
        <p>He et al. (2021) examines adapter-based tuning as a parameter-efficient alternative to fine-tuning for pretrained language models. This method involves adding lightweight adapter modules which are fine-tuned for specific tasks. The study finds that adapter
    </section>

    <section>
        <h2>METHODOLOGY</h2>
        <h3>Understanding Wav2Vec 2.0</h3>
        <p>Wav2Vec 2.0 (wave to vector) is a speech 
            recognition model developed by Facebook AI, 
            designed to learn useful representations of audio data 
            from large amounts of unlabelled speech [42]. The 
            model operates in two primary phases: pretraining 
            and fine-tuning.</p>
        <p>The model is composed of a multi-layer 
            convolutional feature encoder f: X → Z which takes 
            as input raw audio X and outputs latent speech 
            representations z1, . . . , zT for T time-steps. They are 
            then fed to a Transformer g: Z → C to build 
            representations c1, . . . , cT capturing information 
            from the entire sequence [44]. The output of the 
            feature encoder is discretised to qt with a 
            quantization module Z → Q to represent the targets 
            in the self-supervised objective [45].</p>
        <p>In the pretraining phase, wav2Vec 2.0 is exposed to 
            raw audio data without any accompanying 
            transcriptions or labels. The goal of this phase is to 
            learn a general understanding of the audio's structure 
            and patterns [46]. The process begins by dividing the 
            raw audio signal into small segments or frames. 
            These frames are then fed into a multi-layer 
            convolutional neural network (CNN) that extracts 
            low-level features from the audio [47].
            </p>
        <figure>
            <img src="/frontend/assets/wav2vec2_pretraining.png" alt="Visualization of the pretraining phase of the Wav2vec2.0 models">
            <figcaption>Figure 1: Visualization of the pretraining phase of the Wav2vec2.0 models.</figcaption>
        </figure>
        <p>The core innovation of wav2vec 2.0 lies in its 
            contrastive learning approach, where the model 
            learns to distinguish between true and false audio 
            segments. This is achieved by masking a portion of 
            the input audio and predicting the masked parts based 
            on the context provided by the surrounding audio 
            frames [48]. Specifically, the model employs a 
            transformer network to capture long-range 
            dependencies and relationships in the audio data. The 
            output embeddings from the transformer are then 
            used in a contrastive task where the model must 
            identify the correct masked segments from a set of 
            negative samples [49].
            </p>
        <!-- Continue with methodology... -->
    </section>

    <section>
        <h2>RESULTS</h2>
        <h3>Training Setup</h3>
        <p>We began by experimenting with the hyperparameters and the training setup. The training was done using GPUs provided by Google Colab. Initially, we attempted to train both models on the T4 GPU; however, we ran into “Out-Of-Memory” errors because of the large batch size (training parameter) and limited RAM of the GPU.</p>
        <!-- Continue with results... -->
    </section>

    <section>
        <h2>CLOSING</h2>
        <p>In this study, we successfully fine-tuned the Massively Multilingual Speech (MMS) wav2vec 2.0 model to recognize and transcribe the Ika language, demonstrating the feasibility of using pretrained models for low-resource languages. Our approach leveraged just one hour of labeled data to achieve a word error rate (WER) of 0.477553 and a character error rate (CER) of 0.241756 with the 1 billion parameter model.</p>
        <!-- Continue with closing and future work... -->
    </section>

    <section>
        <h2>REFERENCES</h2>
        <ol>
            <li>Conneau, A., et al. (2020). XLSR: Cross-Lingual Speech Representation Learning.</li>
            <!-- Continue with references... -->
        </ol>
    </section>
</body>
</html>   